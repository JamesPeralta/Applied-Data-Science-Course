{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Term Test 2 - Nov 27, 2019 </center></h2>\n",
    "<h2> <center> 100 marks - 2 hours </center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Your Full Name:` \n",
    "\n",
    "`Your Student ID:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1: Classification (35 pts)\n",
    "\n",
    "In this question, you will apply some classification methods on the Breast Cancer dataset, provided by SKLearn. The breast cancer dataset is a classic binary classification dataset that can be found here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "     \n",
    "     \n",
    "\n",
    "After loading the dataset, spilt the data into test and train subsets using K-Fold(k=5) - use random_state=0. Then follow the 3 steps, explianed below:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that has not been mentioned.\n",
    "</div>\n",
    "\n",
    "* Step1:  Build 3 classification models using LinearSVC, GaussianNB, and KNeighborsClassifier from SKLearn, with default settings. Each model gets all features of the dataset as the feature set and predicts the the target as 'malignant' or 'benign'. \n",
    "\n",
    "    * Report your prediction results as three confusion matrices; one matrix per model (representing the mean of 5 folds results per model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesperalta/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jamesperalta/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jamesperalta/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jamesperalta/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jamesperalta/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# load the data\n",
    "cancer_data = load_breast_cancer()\n",
    "features = cancer_data.data\n",
    "labels = cancer_data.target\n",
    "\n",
    "# Split using K-cross validation\n",
    "k_folds = KFold(n_splits=5)\n",
    "k_folds.get_n_splits(features)\n",
    "\n",
    "scores = {\"LinearSVC\": [], \"GaussianNB\": [], \"KNeighbors\": []}\n",
    "for train_index, test_index in k_folds.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # LinearSVC\n",
    "    lin_svc = LinearSVC().fit(X_train, y_train)\n",
    "    scores[\"LinearSVC\"].append(confusion_matrix(lin_svc.predict(X_test), y_test))\n",
    "\n",
    "    # GaussianNB\n",
    "    gaus_nb = GaussianNB().fit(X_train, y_train)\n",
    "    scores[\"GaussianNB\"].append(confusion_matrix(gaus_nb.predict(X_test), y_test))\n",
    "    \n",
    "    # KNeighborsClassifier \n",
    "    k_nb = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    scores[\"KNeighbors\"].append(confusion_matrix(k_nb.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which model(s) has the highest False Postive and which one(s) has the highest False Negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for LinearSVC: [[33.   1.2]\n",
      " [ 9.4 70.2]]\n",
      "Confusion matrix for GaussianNB: [[37.6  2.4]\n",
      " [ 4.8 69. ]]\n",
      "Confusion matrix for KNeighbors: [[36.6  2.6]\n",
      " [ 5.8 68.8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for key in scores.keys():\n",
    "    arr_1 = np.array(scores[key][0])\n",
    "    arr_2 = np.array(scores[key][1])\n",
    "    arr_3 = np.array(scores[key][2])\n",
    "    arr_4 = np.array(scores[key][3])\n",
    "    arr_5 = np.array(scores[key][4])\n",
    "    \n",
    "    print(\"Confusion matrix for {}: {}\".format(key, (arr_1 + arr_2 + arr_3 + arr_4 + arr_5) / 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K-neighbors has the highest amount of false positives\n",
    "Linear SVS has the highest amount of false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2: Normalize your data using StandardScaler. Then build the same classification models (LinearSVC, GaussianNB, and KNN) this time using the scaled data. \n",
    "\n",
    "    * Report your prediction results in the form of three confusion matrices, again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split using K-cross validation\n",
    "k_folds = KFold(n_splits=5)\n",
    "k_folds.get_n_splits(features)\n",
    "\n",
    "scores = {\"LinearSVC\": [], \"GaussianNB\": [], \"KNeighbors\": []}\n",
    "for train_index, test_index in k_folds.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Scale all the features (except label) to map them into [0,1] interval.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # LinearSVC\n",
    "    lin_svc = LinearSVC().fit(X_train, y_train)\n",
    "    scores[\"LinearSVC\"].append(confusion_matrix(lin_svc.predict(X_test), y_test))\n",
    "\n",
    "    # GaussianNB\n",
    "    gaus_nb = GaussianNB().fit(X_train, y_train)\n",
    "    scores[\"GaussianNB\"].append(confusion_matrix(gaus_nb.predict(X_test), y_test))\n",
    "    \n",
    "    # KNeighborsClassifier \n",
    "    k_nb = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    scores[\"KNeighbors\"].append(confusion_matrix(k_nb.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for LinearSVC: [[40.6  1.8]\n",
      " [ 1.8 69.6]]\n",
      "Confusion matrix for GaussianNB: [[37.8  3.4]\n",
      " [ 4.6 68. ]]\n",
      "Confusion matrix for KNeighbors: [[39.   1.2]\n",
      " [ 3.4 70.2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for key in scores.keys():\n",
    "    arr_1 = np.array(scores[key][0])\n",
    "    arr_2 = np.array(scores[key][1])\n",
    "    arr_3 = np.array(scores[key][2])\n",
    "    arr_4 = np.array(scores[key][3])\n",
    "    arr_5 = np.array(scores[key][4])\n",
    "    \n",
    "    print(\"Confusion matrix for {}: {}\".format(key, (arr_1 + arr_2 + arr_3 + arr_4 + arr_5) / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Identify which model(s) has most improved with scaling. Explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now the Gaussian NB has the most false positive and false negatives\n",
    "The linear SVC improves the most with scaling because it just tries to fit a straight line.\n",
    "When the variance is lower it will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: Combine the above three models using hard vote in VotingClassifier. Use the same Scaled (normalized) data as step 2. \n",
    "\n",
    "    * Again, report your prediction result in the form of confusion matrix (this time, one matrix). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "# Split using K-cross validation\n",
    "k_folds = KFold(n_splits=5)\n",
    "k_folds.get_n_splits(features)\n",
    "\n",
    "scores = {\"Voting\": []}\n",
    "for train_index, test_index in k_folds.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Scale all the features (except label) to map them into [0,1] interval.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # LinearSVC\n",
    "    lin_svc = LinearSVC()\n",
    "    # GaussianNB\n",
    "    gaus_nb = GaussianNB()\n",
    "    # KNeighborsClassifier \n",
    "    k_nb = KNeighborsClassifier()\n",
    "    \n",
    "    vote = VotingClassifier(estimators=[('lin_svc', lin_svc), ('gaus_nb', gaus_nb), ('k_nb', k_nb)], voting=\"hard\")\n",
    "    vote.fit(X_train, y_train)\n",
    "    scores[\"Voting\"].append(confusion_matrix(vote.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for KNeighbors: [[39.2  0.8]\n",
      " [ 3.2 70.6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr_1 = np.array(scores[\"Voting\"][0])\n",
    "arr_2 = np.array(scores[\"Voting\"][1])\n",
    "arr_3 = np.array(scores[\"Voting\"][2])\n",
    "arr_4 = np.array(scores[\"Voting\"][3])\n",
    "arr_5 = np.array(scores[\"Voting\"][4])\n",
    "\n",
    "print(\"Confusion matrix for {}: {}\".format(key, (arr_1 + arr_2 + arr_3 + arr_4 + arr_5) / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is there any improvement using the combined model compared to step2 results? If so, explain where you see improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-69-b0b3c21b3d29>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-69-b0b3c21b3d29>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Yes it is has less false positives\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Yes it is has less false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Regression (40 pts)\n",
    "\n",
    "In this question, you're asked to build a regression model to predict the price of a new house based on the data of previously sold houses that is provided in a dataset from California Housing Prices.\n",
    "\n",
    "#### Part A. Random Forest Regression  \n",
    "As a first step, train a simple RandomForestRegressor model by following these steps:\n",
    "\n",
    "* Load the data from here: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n",
    "* Split it into train (2/3) and test (1/3), with random_state=42. \n",
    "\n",
    "* Scale all the features (except label) to map them into [0,1] interval. \n",
    "* Use the scaled training set to build a basic RandomForestRegressor model with n_estimators=100 and random_state=0.\n",
    "* Report the score (R^2) of your model on train and test set. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that has not been mentioned.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7187034094458631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# load the data\n",
    "house_data = fetch_california_housing()\n",
    "features = house_data.data\n",
    "labels = house_data.target\n",
    "\n",
    "# Split it into train (2/3) and test (1/3), with random_state=42.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=42, train_size=0.66)\n",
    "\n",
    "# Scale all the features (except label) to map them into [0,1] interval.\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Use the scaled training set to build a basic RandomForestRegressor model with n_estimators=100 and random_state=0.\n",
    "regr = RandomForestRegressor(random_state=0, n_estimators=10).fit(X_train, y_train)\n",
    "\n",
    "# Report the score (R^2) of your model on train and test set.\n",
    "score = r2_score(regr.predict(X_test), y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. Improve the model\n",
    "\n",
    "Now try to improve the model by tuning the parameters:\n",
    "\n",
    "* Use GridSearchCV (with 5-fold cross-validation to make the train and validation sets) to tune part A's RandomForestRegressor's parameters as follows: \n",
    "    * max_features with 'auto', 'sqrt', and 'log2' \n",
    "    * max_depth with None, 10, 20, and 30\n",
    "    * min_samples_split with 2, 5, and 10\n",
    "\n",
    "* Report the tuned values of parameters as well as the score of the tuned model, on both train and test sets.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "Use the same scaled data and the same train, test split as Part A. Also for any unmentioned paramter, use the same default as Part A.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesperalta/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\"\"\"\n",
    "Use GridSearchCV (with 5-fold cross-validation to make the train and validation sets)\n",
    "to tune part A's RandomForestRegressor's parameters as follows:\n",
    "\"\"\"\n",
    "# Create search grid parameters and classifier\n",
    "regr = RandomForestRegressor(random_state=0, n_estimators=10)\n",
    "parameters = {'max_features': ('auto', 'sqrt', 'log2'), 'max_depth': (None, 10, 20, 30), 'min_samples_split': (2, 5, 10)}\n",
    "clf = GridSearchCV(regr, parameters, cv=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best classifier here for re-use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C. Dimension Reduction\n",
    "\n",
    "Reduce the dimensionality of the data set to 4 dimensions, using PCA and RFE and repeat the predictions and report the scores. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "Make sure you use the same set up as Part B. That is the same train, test split, the same scaled data, and the same tuned values (i.e., you just take the one best tuned model and no need to do the corss-validation tuning again). \n",
    "</div>\n",
    "\n",
    "__PCA__:\n",
    "\n",
    "* Use PCA to reduce the features dimensions to 4 components. Note that knowing where and when PCA must be applied, is part of the question.\n",
    "* Now use the new feature set (4 PCA components rather than the original features) and report the R^2 score on the test set, using the same tuned  RandomForestRegressor model as part B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4).fit(X_train)\n",
    "pca_X_train = pca.transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Train lin regr and report score here\n",
    "regr = RandomForestRegressor(random_state=0, best_params).fit(pca_X_train, pca_X_test)\n",
    "print(regr.score(regr.predict(pca_X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RFE__:\n",
    "\n",
    "* Use RFE with a simple linear regression model (using default values) to reduce the data to its 4 most informative features. Note that knowing how to properly apply RFE, is part of the question.\n",
    "* Now use the new feature set (top 4 features rather than the original features) and report the R^2 score on the test set, using the same tuned  RandomForestRegressor model as part B. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RFE and report score here\n",
    "from sklearn.feature_selection import RFE\n",
    "import numpy as np\n",
    "\n",
    "RFEselect = RFE(RandomForestRegressor(random_state=0, best_params),n_features_to_select=4).fit(X_train, y_train)\n",
    "# select the right features\n",
    "print(\"Features selected: **{}**\".format([list(features.columns.values)[i] for i in np.where(mask == True)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptually, what is the key difference between dimension reduction using RFE and PCA. \n",
    "* Explian your observation on the regresion results of RFE and PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The key difference between RFE and PCA is the RFE does not change transform the features value. \n",
    "RFE just chooses which features to include in the model. Where PCA will completely change the values\n",
    "that are passed into the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Text Analysis (25 pts)\n",
    "\n",
    "In this question, you are given a dataset of movie_plots and asked to build an LDA (Latent Dirichlet Allocation) model to extract different topics that exist among these plots, by following these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Read the given \"movie_plots.csv\" file and load it to a pandas dataframe.\n",
    "- Vectorize the plots (provided in the \"Plot\" feature) using CountVectorizer, while considering the followings:\n",
    "    - use only bi-grams tokens. \n",
    "    - ignore english_stop_words.\n",
    "    - keep the maximum number of features at 10,000.\n",
    "    - drop features that appear in more than 5% of movies.\n",
    "    - drop features that appear in less than 10 movies.\n",
    "- Next build an LDA model (random_state=0) with 10 topics and fit it with all the vectorized movie plots.   \n",
    "- Now print the topics, using the 5 most important words per topics\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that have not been mentioned.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "movie_data = pd.read_csv(\"movie_plots.csv\")\n",
    "movie_data[\"Plot\"]\n",
    "\n",
    "# Your solution\n",
    "def build_lda(X_train):\n",
    "    \"\"\"\n",
    "    Create a count vectorizer as described above, feed the vectors into an LDA and fit it. \n",
    "    It should return both the lda and count vectorizer as they're both going to be used later. \n",
    "    Just keep the return statement as is and you should be good to go.\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(min_df=10, max_df=0.05,ngram_range= (2,2), max_features=10000, stop_words=\"english\")\n",
    "    X_train = count_vectorizer.fit_transform(X_train)\n",
    "    lda = LatentDirichletAllocation(n_components=25, random_state=0)\n",
    "    lda = lda.fit(X_train)\n",
    "    return lda, count_vectorizer\n",
    "\n",
    "lda_bi, cv = build_lda(movie_data[\"Plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top in 1: st loui drive away secur guard park lot drug dealer\n",
      "Top in 2: san francisco attempt kill month later ransom money young boy\n",
      "Top in 3: home run secret servic hotel room fbi agent grand central\n",
      "Top in 4: new orlean car accid gang member ice cream begin work\n",
      "Top in 5: young man new mexico month later tri kill life support\n",
      "Top in 6: fbi agent old friend las vega phone sex best man\n",
      "Top in 7: air forc white hous nuclear weapon polar bear doe want\n",
      "Top in 8: tri kill cell phone special agent tooth fairi manag escap\n",
      "Top in 9: hotel room spend night young woman new life film follow\n",
      "Top in 10: white hous vice presid washington dc month later young man\n",
      "Top in 11: real world old man make way fbi agent look like\n",
      "Top in 12: year ago month later time later hotel room tri convinc\n",
      "Top in 13: time travel gas station make way day later just time\n",
      "Top in 14: prime minist space station crew member gang member steal money\n",
      "Top in 15: las vega serial killer fbi agent make way crime scene\n",
      "Top in 16: month later film open hotel room drive away make way\n",
      "Top in 17: secur guard young woman tri convinc san francisco gas station\n",
      "Top in 18: old man north pole tell stori santa claus year earlier\n",
      "Top in 19: blind man follow day young man tell stori locker room\n",
      "Top in 20: new life ray ray year ago start new las vega\n",
      "Top in 21: make way las vega young man chang mind later night\n",
      "Top in 22: month later drive away young woman make way look like\n",
      "Top in 23: make way doesnt want news report old friend walk away\n",
      "Top in 24: new jersey make way old friend famili friend las vega\n",
      "Top in 25: white hous year ago drive away manag escap tri help\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda_bi.components_, start=1):\n",
    "    sorted_topics = np.argsort(topic)[::-1]\n",
    "    feature_names = np.array(cv.get_feature_names())\n",
    "    \n",
    "    top_five = feature_names[sorted_topics][:5]\n",
    "    top_five = \" \".join(top_five)\n",
    "    \n",
    "    print('Top in '+str(i)+':', top_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now add uni-gram and tri-gram tokens to the bi-gram tokens, and keep everything else the same and redo the LDA modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n",
    "def build_lda(X_train):\n",
    "    \"\"\"\n",
    "    Create a count vectorizer as described above, feed the vectors into an LDA and fit it. \n",
    "    It should return both the lda and count vectorizer as they're both going to be used later. \n",
    "    Just keep the return statement as is and you should be good to go.\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(min_df=10, max_df=0.05,ngram_range= (1,3), max_features=10000, stop_words=\"english\")\n",
    "    X_train = count_vectorizer.fit_transform(X_train)\n",
    "    lda = LatentDirichletAllocation(n_components=25, random_state=0)\n",
    "    lda = lda.fit(X_train)\n",
    "    return lda, count_vectorizer\n",
    "\n",
    "lda_all, cv = build_lda(movie_data[\"Plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(lda_all.components_, start=1):\n",
    "    sorted_topics = np.argsort(topic)[::-1]\n",
    "    feature_names = np.array(cv.get_feature_names())\n",
    "    \n",
    "    top_five = feature_names[sorted_topics][:5]\n",
    "    top_five = \" \".join(top_five)\n",
    "    \n",
    "    print('Top in '+str(i)+':', top_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - What major changes you see in the resulting topics?  Explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The PCA didn't have enough time to finish so I couldn't submit this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
