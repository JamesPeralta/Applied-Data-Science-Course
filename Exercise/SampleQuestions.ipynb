{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Sample questions on feature engineering and model evaluation</center></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part A. Feature Selection </h1>\n",
    "<br>\n",
    "\n",
    "In this section, we are going to select the most informative features from the given NASA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Part A.1. Feature selection using ANOVA </h2>\n",
    "\n",
    "Steps:\n",
    "    \n",
    "- Read data from the NASA.csv\n",
    "- Using ANOVA select top K features, whith K=[1..10]\n",
    "- Build LogisticRegression models, one with the original data, and one for each K (using a subset of feature)\n",
    "- Compare the accuracies and find the best K (based on the median of 30 runs with random_state=[0 to 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "## Part A.1 ANOVA \n",
    "nasa_data = pd.read_csv(\"NasaData.csv\")\n",
    "features = nasa_data.drop(columns=\"label\")\n",
    "labels = nasa_data[\"label\"]\n",
    "\n",
    "scores = {\"log_reg_{}\".format(i):[] for i in range(0, 11)}\n",
    "# Test regular logistic regression\n",
    "for split_seed in range(0, 30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        labels,\n",
    "        random_state=split_seed\n",
    "    )\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "    scores[\"log_reg_0\"].append(lr.score(X_test, y_test))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    for split_seed in range(0, 30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features,\n",
    "            labels,\n",
    "            test_size=0.2,\n",
    "            random_state=split_seed\n",
    "        )\n",
    "        \n",
    "        # Select K best features using ANOVA\n",
    "        k_best = SelectKBest(k=i)\n",
    "        k_best.fit(X_train, y_train)\n",
    "        \n",
    "        X_train = k_best.transform(X_train)\n",
    "        X_test = k_best.transform(X_test)\n",
    "    \n",
    "        lr = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "        scores[\"log_reg_{}\".format(i)].append(lr.score(X_test, y_test))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median accuracy of log_reg_0 is 0.7869989722507709\n",
      "The median accuracy of log_reg_1 is 0.7861271676300579\n",
      "The median accuracy of log_reg_2 is 0.7874116891457932\n",
      "The median accuracy of log_reg_3 is 0.7861271676300579\n",
      "The median accuracy of log_reg_4 is 0.7864482980089917\n",
      "The median accuracy of log_reg_5 is 0.7864482980089917\n",
      "The median accuracy of log_reg_6 is 0.7861271676300579\n",
      "The median accuracy of log_reg_7 is 0.7867694283879255\n",
      "The median accuracy of log_reg_8 is 0.7861271676300579\n",
      "The median accuracy of log_reg_9 is 0.7854849068721901\n",
      "The median accuracy of log_reg_10 is 0.7854849068721901\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in scores.keys():\n",
    "    print(\"The median accuracy of {} is {}\".format(i, np.median(scores[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A.2. Compare feature selection models </h2>\n",
    "\n",
    "Now apply SelectFromModel and RFE and compare them with SelectKBest, as follows:\n",
    "\n",
    "- Apply the three techniques so that you reduce the features to only 6 features (note that 6 is not necessarily the best K from Part A.1)\n",
    "- Report the prediction scores of a LogisticRegression model on the selected features of each model.\n",
    "- Print the name of features selected by each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7929085303186023\n",
      "0.7934224049331963\n",
      "0.7959917780061665\n"
     ]
    }
   ],
   "source": [
    "##  Part A.2 Compare feature selection models \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    labels,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Create select from model feature selector \n",
    "select = SelectFromModel(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    threshold=\"1.25*mean\")\n",
    "select.fit(X_train, y_train)\n",
    "select_X_train = select.transform(X_train)\n",
    "select_X_test = select.transform(X_test)\n",
    "print(LogisticRegression(random_state=0).fit(select_X_train, y_train).score(select_X_test, y_test))\n",
    "\n",
    "# Create RFE feature selector\n",
    "rfe_select = RFE(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    n_features_to_select=6\n",
    ")\n",
    "rfe_select.fit(X_train, y_train)\n",
    "rfe_select_X_train = rfe_select.transform(X_train)\n",
    "rfe_select_X_test = rfe_select.transform(X_test)\n",
    "print(LogisticRegression(random_state=0).fit(rfe_select_X_train, y_train).score(rfe_select_X_test, y_test))\n",
    "\n",
    "# Select 6 best features using ANOVA\n",
    "k_best = SelectKBest(k=6)\n",
    "k_best.fit(X_train, y_train)\n",
    "k_best_X_train = k_best.transform(X_train)\n",
    "k_best_X_test = k_best.transform(X_test)\n",
    "print(LogisticRegression(random_state=0).fit(k_best_X_train, y_train).score(k_best_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected: **['LOC_BLANK', 'BRANCH_COUNT', 'LOC_CODE_AND_COMMENT', 'LOC_COMMENTS', 'CYCLOMATIC_COMPLEXITY', 'DESIGN_COMPLEXITY', 'ESSENTIAL_COMPLEXITY', 'LOC_EXECUTABLE', 'HALSTEAD_CONTENT', 'HALSTEAD_DIFFICULTY', 'HALSTEAD_EFFORT', 'HALSTEAD_ERROR_EST', 'HALSTEAD_LENGTH', 'HALSTEAD_LEVEL', 'HALSTEAD_PROG_TIME', 'HALSTEAD_VOLUME', 'NUM_OPERANDS', 'NUM_OPERATORS', 'NUM_UNIQUE_OPERANDS', 'NUM_UNIQUE_OPERATORS', 'LOC_TOTAL']**\n"
     ]
    }
   ],
   "source": [
    "mask = rfe_select.get_support()\n",
    "print(\"Features selected: **{}**\".format([list(features.columns.values)[i] for i in np.where(mask == True)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part B. Data Tranformation </h1>\n",
    "\n",
    "In this part, you are going to work with a new data set which contains some the features of a house collected over time. \n",
    "The objective of this part is to help improve linear model's predicitons using data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B.1 Binning </h2>\n",
    "\n",
    "Our first try is using binning, as follows:\n",
    "\n",
    "- Read from MyHouse.csv (take 'Light' as the data target and the rest of the columns as data features ) \n",
    "- First apply a LinearRegression on the original data to predict the target and report the score of the model on the test set. \n",
    "- Now apply binning on all three columns \n",
    " (for Temperature make 5 Bins -- for Humidity make 10 bins -- and for CO2Bins make 11 bins)\n",
    "- Print your data shape before and after binning.\n",
    "- Now again apply LinearRegression on the new data and report the score again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of original lin reg = 0.6920532277961199\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[21.025      21.5        20.7        ... 20.39       22.6\n 22.82333333].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3b1ac8d1b446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Apply binning to each column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtemp_binner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKBinsDiscretizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Temperature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mhumidity_binner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKBinsDiscretizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Humidity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mco2_binner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKBinsDiscretizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CO2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'numeric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mvalid_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'onehot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'onehot-dense'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ordinal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PythonEnvs/ENSF-519/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[21.025      21.5        20.7        ... 20.39       22.6\n 22.82333333].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "house_data = pd.read_csv(\"MyHouse.csv\")\n",
    "features = house_data.drop(columns=\"Light\")\n",
    "labels = house_data[\"Light\"]\n",
    "\n",
    "house_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    labels,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "print(\"Score of original lin reg = {}\".format(lin_reg.score(X_test, y_test)))\n",
    "\n",
    "# Apply binning to each column\n",
    "temp_binner = KBinsDiscretizer(n_bins=5).fit(X_train[\"Temperature\"])\n",
    "humidity_binner = KBinsDiscretizer(n_bins=10).fit(X_train[\"Humidity\"])\n",
    "co2_binner = KBinsDiscretizer(n_bins=11).fit(X_train[\"CO2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B.2 Polynomials</h2>\n",
    "\n",
    "To compare polynomials and binning, apply polynomials on all three features. \n",
    "- Use degree=6.\n",
    "- Print your data shape before and after transformation\n",
    "- Apply LinearRegression on the new data and report the score again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part B.2 Compare feature selection models \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
