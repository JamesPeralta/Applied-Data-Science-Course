{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 5** - 100 marks\n",
    "\n",
    "**Due:** November 25, 05.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you are done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam detection \n",
    "\n",
    "Here we have a dataset of text messages which are labeled as spam or ham. We want to read the dataset and use a clustering algorithm to tell spam messages from non-spam (ham!) ones. The data is in tsv format with two columns: label and text. TSV is just like csv but the column values are separated by a tab instead of a `,`. \n",
    "\n",
    "1. Read the file into a dataframe\n",
    "2. Convert `label` column to pandas categorical data type\n",
    "3. complete the `clean_text` function and apply it to the text column. To clean up:\n",
    "    1. Make it lowercase\n",
    "    2. Remove all of the punctuations (use `string.punctuation` and `str.translate`)\n",
    "    3. Replace repetetive whitespaces with just one blank charachter (e.g.: 'i    had \\tan apple' -> 'i had an apple')\n",
    "    4. Removing the stop words\n",
    "    5. Stem each word using snowball stemmer provided in `nltk`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    object\n",
      "text     object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>jurong point crazi avail bugi n great world la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>nah don t think goe usf live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>spam</td>\n",
       "      <td>2nd time tri 2 contact u u won £750 pound priz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>ham</td>\n",
       "      <td>ü b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>ham</td>\n",
       "      <td>piti mood suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>ham</td>\n",
       "      <td>guy did bitch act like d interest buy week gav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>ham</td>\n",
       "      <td>rofl true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0      ham  jurong point crazi avail bugi n great world la...\n",
       "1      ham                              ok lar joke wif u oni\n",
       "2     spam  free entri 2 wkli comp win fa cup final tkts 2...\n",
       "3      ham                        u dun say earli hor u c say\n",
       "4      ham                       nah don t think goe usf live\n",
       "...    ...                                                ...\n",
       "5567  spam  2nd time tri 2 contact u u won £750 pound priz...\n",
       "5568   ham                            ü b go esplanad fr home\n",
       "5569   ham                                  piti mood suggest\n",
       "5570   ham  guy did bitch act like d interest buy week gav...\n",
       "5571   ham                                          rofl true\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    text: str, returns: str\n",
    "    \"\"\"\n",
    "    # Make text lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove all of the punctuations\n",
    "    translation = str.maketrans(string.punctuation, \" \"*len(string.punctuation))\n",
    "    text = text.translate(translation)\n",
    "    \n",
    "    # Replace repetetive which space with just one blank character\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Remove stopping words\n",
    "    text = \" \".join(list(filter(lambda x: x not in ENGLISH_STOP_WORDS, text.split(\" \"))))\n",
    "    \n",
    "    # Stem each word using the snowball stemmer\n",
    "    text = \" \".join(list(map(lambda x: stemmer.stem(x), text.split(\" \"))))\n",
    "    return text\n",
    "\n",
    "\n",
    "sms = pd.read_csv('sms.tsv', sep='\\t', names=['label', 'text'])\n",
    "sms[\"text\"] = sms[\"text\"].apply(clean_text)\n",
    "print(sms.dtypes)  # Expected: label category text object dtype: object\n",
    "sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test sets (20% test 80% training), use `stratify` parameter to ensure that there is an even split for both categories. X values should be the vectors and y values the labels.\n",
    "\n",
    "\n",
    "Complete `build_tfidf` function below to make and fit a TF-IDF vectorizer with `min_df` = 2. This function is later called in the loop. \n",
    "\n",
    "Similar to previous assignments and problems, change the random state in a for loop from 0 to 9, in each iteration:\n",
    "\n",
    "1. The vectorizer instance is created on the training set. Convert the `X_train` and `X_test` by calling the `transform` method. You don't have to refit the transformer in the loop.\n",
    "2. train and evaluate these classifiers: \n",
    "\n",
    "    * LogisticRegression \n",
    "    * LinearSVC\n",
    "    * Naïve Bayes, with Bernoulli distribution\n",
    "    * Decision tree, use 20 for `min_samples_split` to prevent overfitting\n",
    "    * Random Forest, with a 100 estimators and use min_samples_split like above\n",
    "\n",
    " \n",
    "\n",
    "Use `random_state`s from in `[0, 5)` in classifier contruction for those which accept this parameter.\n",
    "Keep record of these scores in a pandas dataframe and make a boxplot to compare them. Set ylim to `(0.85, 1)`. \n",
    "\n",
    "**You should do all these inside the `fit_eval` and `build_tfidf` functions. Don't add any lines of code before or after it.** Also it doesn't need to return any values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def build_tfidf(X_train):\n",
    "    vectorizer = TfidfVectorizer(min_df=2).fit(X_train)\n",
    "    # Fit the vectorizer here\n",
    "    return (vectorizer,)  # A tuple with only one item\n",
    "\n",
    "\n",
    "def fit_eval(get_vectorizer):\n",
    "    # Column names: 'Logistic Regression', 'Linear SVC', 'Naïve Bayes', 'Decision tree', 'Random Forest'\n",
    "    results = []\n",
    "    for split_seed in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(sms[\"text\"], sms[\"label\"], random_state=split_seed ,train_size=0.8, stratify=sms[\"label\"])\n",
    "        vectorizer = get_vectorizer(X_train)[0]\n",
    "        # Apply the transformations using `vectorizer` instance above\n",
    "        X_train = vectorizer.transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "        \n",
    "        for ran_state in range(0, 5):\n",
    "            scores = {}\n",
    "            # LogisticRegression\n",
    "            lr = LogisticRegression(random_state=ran_state, solver='lbfgs').fit(X_train, y_train)\n",
    "            lr_score = lr.score(X_test, y_test)\n",
    "            scores[\"Log_Reg\"] = lr_score\n",
    "\n",
    "            # LinearSVC\n",
    "            lsvc = LinearSVC(random_state=ran_state).fit(X_train, y_train)\n",
    "            lsvc_score = lsvc.score(X_test, y_test)\n",
    "            scores[\"Lin_SVC\"] = lsvc_score\n",
    "\n",
    "            # Naïve Bayes, with Bernoulli distribution\n",
    "            nv_bd = BernoulliNB().fit(X_train, y_train)\n",
    "            nv_bd_score = nv_bd.score(X_test, y_test)\n",
    "            scores[\"Naive_Bayes\"] = nv_bd_score\n",
    "\n",
    "            # Decision tree, use 20 for min_samples_split to prevent overfitting\n",
    "            dtc = DecisionTreeClassifier(random_state=ran_state, min_samples_split=20).fit(X_train, y_train)\n",
    "            dtc_score = dtc.score(X_test, y_test)\n",
    "            scores[\"Decision_Tree\"] = dtc_score\n",
    "            \n",
    "            # Random Forest, with a 100 estimators and use min_samples_split like above\n",
    "            rfc = RandomForestClassifier(n_estimators=100, min_samples_split=20, random_state=ran_state).fit(X_train, y_train)\n",
    "            rfc_score = rfc.score(X_test, y_test)\n",
    "            scores[\"Random_Forest\"] = rfc_score\n",
    "            \n",
    "            # Append all scores to the results\n",
    "            results.append(scores)\n",
    "\n",
    "    # Should not return anything\n",
    "    pd.DataFrame(results).boxplot().set_ylim(0.85, 1)\n",
    "\n",
    "fit_eval(build_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `fit_eval` function again but this time instead of using the TF-IDF vectors directly, we make a new function that builds, fits, and returns: a LDA with 25 topics and use topic coverage vector of each document (text message). Use 0 as the `random_state` and a `CountVectorizer` with `min_df` = 2 to vectorize the text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Your solution\n",
    "def build_lda(X_train):\n",
    "    \"\"\"\n",
    "    Create a count vectorizer as described above, feed the vectors into an LDA and fit it. \n",
    "    It should return both the lda and count vectorizer as they're both going to be used later. \n",
    "    Just keep the return statement as is and you should be good to go.\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(min_df=2)\n",
    "    X_train = count_vectorizer.fit(X_train)\n",
    "    lda = LatentDirichletAllocation(n_components=25, random_state=0)\n",
    "    lda = lda.fit_transform(X_train)\n",
    "    return lda, count_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit_eval(build_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `components_` attribute of the LDA to find top 5 words of each topic and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _,_,_ = train_test_split(sms['text'], sms['label'], random_state=0, stratify=...# The same as fit_eval)  \n",
    "lda, cv = build_lda(X_train)\n",
    "\n",
    "for i, topic in enumerate(lda.components_, start=1):\n",
    "    # Your solution\n",
    "    print('Top in '+str(i)+':', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences describe your observations with regard to comparing vanilla TF-IDF vs adding a LDA on top of a count vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit `enter` to edit this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
