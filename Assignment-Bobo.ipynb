{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 1** - 100 marks\n",
    "\n",
    "**Due:** October 4th, 04.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you were done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - The Zipf mystery (50 points)\n",
    "\n",
    "In this problem, we'd like to read the text from a book and perform some simple statistical analysis on the word counts. We have provided you with the actual text from [Lost On The Moon or, In Quest of the Field of Diamonds](https://www.goodreads.com/book/show/8636132-lost-on-the-moon-or-in-quest-of-the-field-of-diamonds) book in a file named 'the book.txt'. The file is cleaned up and only contains alphanumeric characters, i.e. no punctuation, quotation marks, etc.\n",
    "\n",
    "Read the file and break it down to its words. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def read_and_tokenize(file_name):\n",
    "    # Your solution\n",
    "    words = []\n",
    "    \n",
    "    try:\n",
    "        fileHandle = open(file_name, \"r\")        \n",
    "        fileData = fileHandle.read()\n",
    "        words = fileData.split()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Invalid file name\")\n",
    "        \n",
    "    return words\n",
    "\n",
    "words = read_and_tokenize('the book.txt')\n",
    "words[1101:1111] # Expected: ['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sorted list of unique words in the book. Store the list in a variable called `V`. Also complete the `get_word_index` function below that gets a word and finds its index within `V`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here\n",
    "wordsSet = sorted(set(words))\n",
    "V = list(wordsSet)\n",
    "\n",
    "def get_word_index(word):\n",
    "    return V.index(word)\n",
    "\n",
    "get_word_index('about')  # Expected: 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using no loops, and by only using `map` and `filter` built-in python functions traverse through the `V` (vocabulary) list above to find:\n",
    "\n",
    "* `long_words`: The list of words that have 10 letters or more \n",
    "* `no_vowels`: A list of all words but with vowels (aoeiu) removed. You can nest `map` and `filter` calls to iterate through the characters of the words.\n",
    "\n",
    "(5+5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "long_words = list(filter(lambda x: len(x) > 10, V))\n",
    "no_vowels =  list(map(lambda x: re.sub(r'[aeiou]', '', x) , V))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy array of size `|V|` that only contains 0s. Store it in a variable named `frequencies`. Use this array to count the number of times each word has appeared in the book. For example `frequencies[9]` should store how many times the word located in the index 9 of `V` (the sorted list) --which is the word \"about\"-- has been appreaed in the book (165 times). (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1., ..., 11.,  1.,  1.]), 165.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Your solution\n",
    "frequencies = np.zeros(len(V)) \n",
    "\n",
    "def findFreq(words):\n",
    "    wordCount = dict()\n",
    "    for word in words:\n",
    "        wordCount[word] = wordCount.get(word, 0) + 1\n",
    "    \n",
    "    return wordCount\n",
    "\n",
    "#First find frequency of all words in book\n",
    "wordCount = findFreq(words)      \n",
    "for word, i in zip(V, range(len(V))):\n",
    "    frequencies[i] = wordCount[word] \n",
    "    \n",
    "frequencies, frequencies[9] # Expected: array([ 1.,  1.,  1., ..., 11.,  1.,  1.]), 165.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word that appeared most frequently in the book. Find the word itself as well as the number of times it was repeated in the book. Use numpy functions, i.e. do not iterate over the `frequencies` array manually using a `for` loop. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3742\n",
      "\"the\" is the most common word which has appeared 3237.0 times in the book.\n"
     ]
    }
   ],
   "source": [
    "# Your solution \n",
    "maxIdx = np.argmax(frequencies)\n",
    "print(maxIdx)\n",
    "\n",
    "most_common_word = V[maxIdx]\n",
    "max_frequency = frequencies[maxIdx]\n",
    "\n",
    "print(f'\"{most_common_word}\" is the most common word which has appeared {max_frequency} times in the book.')\n",
    "# Expected: \"the\" is the most common word which has appeared 3237 times in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all frequency values by dividing them by the maximum frequency value (using vectorized operators). After this the most common word in the book should get a normalized frequency of `1` and all other words get some value \n",
    "between `1/MAX` and `1`. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00030893, 0.00030893, 0.00030893, ..., 0.00339821, 0.00030893,\n",
       "       0.00030893])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution\n",
    "normalized_frequencies = frequencies / frequencies.max()\n",
    "normalized_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check if the normalized frequencies have any corelation to their ranks. If such correlation exists, the Zipf's law states that it is linear in a log-log space. Take the logarithm of normalized frequencies (as y values) and create a numpy array of the same size containing the rank of each word (as x values). For example if the frequencies array is `[0.1, 1, 0.01, 0.0001]` the x and y values will be `X = [2, 1, 3, 4] Y=[-1, 0, -2, -4]`. \n",
    "\n",
    "You might want to sort the normalized frequencies first to make the task easier. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1 1949 1955 ... 3796  119 1344]\n",
      "[-8.08240225 -8.08240225 -8.08240225 ... -5.68450698 -8.08240225\n",
      " -8.08240225]\n"
     ]
    }
   ],
   "source": [
    "# Your solution \n",
    "normalized_frequencies_sorted = normalized_frequencies.argsort()\n",
    "\n",
    "#for freq, pos in zip(unique_freq, range(len()))\n",
    "x = np.arange(1, len(normalized_frequencies) + 1)[normalized_frequencies_sorted.argsort()]\n",
    "y = np.log(normalized_frequencies)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) on this data. The result is expected to be close to -1. Define appropriate functions for the the statistical calculations as neccessary. Additionally, you can use `pearsonr` function from `scipy` package to check if the calculated value is definitely correct. Though if you get a value close enough to -1 you can almost be sure that your implementation is correct and this step won't be necessary. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated pcc val =  0.863 actual pcc val =  0.863\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Your solution goes here\n",
    "def pcc(x, y):\n",
    "    if len(x) != len(y) or len(x) < 2:\n",
    "        print('pcc cannot be performed on vectors of unequal lengths')\n",
    "        return None\n",
    "    else:   \n",
    "        covariance_matrix = np.cov(x, y)\n",
    "        covariance = covariance_matrix[0][1]\n",
    "        pearson_coeff = covariance / (np.std(x) * np.std(y))\n",
    "        return pearson_coeff\n",
    "\n",
    "calculated_coeff = pcc(x, y)\n",
    "actual_coeff = pearsonr(x, y)\n",
    "print('Calculated pcc val = ', round(calculated_coeff, 3), \n",
    "      'actual pcc val = ', round(actual_coeff[0], 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Log processing (50 points)\n",
    "\n",
    "In this part of the assignment we are going to use regular expressions to mine data out of some webserver log files. Although these problems can be solved without use of RegExes, but for this assignment you need to use them.\n",
    "\n",
    "A sample web server log file is provided along with this problem. In each line of the file one event is recorded. For simplicity all of the events in this file have the same format and are of the same type. Each event contains an ip address, date and time of the event, http method (`GET` or `POST`), a url, HTTP version, HTTP response code (usually 200), the response size in bytes, and the device's user agent which contains information about the device such as the brand and the operating system.\n",
    "\n",
    "Since these logs have such a well defined format regular expressions are the prefect tool for breaking them down into parts and perform different analysis on them.\n",
    "\n",
    "**Please make sure that when you are asked to write a function that _return_s something, you are _return_ing that value, not just _print_ing it**\n",
    "\n",
    "We start off with a random log line and write python functions that use regular expressions to break it off to pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "l = '5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"'\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the ip address part of the log line using regular expressions. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.106.145.204\n"
     ]
    }
   ],
   "source": [
    "def get_ip_address(l):\n",
    "    # Your solution here\n",
    "    ip_address = re.search('^(\\d|\\.)+(?= - -)', l)\n",
    "    return ip_address.group()\n",
    "\n",
    "ip_address = get_ip_address(l)  # Expected: '5.106.145.204'\n",
    "print(ip_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the HTTP method, url, response code, and response size and returns a tuple. Use regular expressions. The http method is either `POST` or `GET` and the response code is always a 3 digit integer. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('POST', '/v1/crash-report/incident/report/', 200, 65)\n"
     ]
    }
   ],
   "source": [
    "def get_http_info(l):\n",
    "    # Your solution here\n",
    "    http_method = re.search('(?<=\\] \")\\S+(?= /)', l).group()\n",
    "    url = re.search('((?<=POST )|(?<=GET ))\\S+(?= HTTP/)', l).group()\n",
    "    resp_sz = re.search('(?<=\\d{3} )\\d+(?=( \"-\" )|( \"))', l).group()\n",
    "    resp_code = re.search('\\d{3}(?= ' + resp_sz + ' )', l).group()\n",
    "    \n",
    "    return (http_method, url, int(resp_code), int(resp_sz))\n",
    "\n",
    "print(get_http_info(l))  # Expected: ('POST', '/v1/crash-report/incident/report/', 200, 65)\n",
    "# Please note that the last two numbers are converted to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions to break the date and time section apart and create a python datetime object based on that. Mind the time zone. convert the datetimes to MDT. Using `strptime` is a better solution in general, but for this assignment please stick to writing RegExes so you become more comfortable in writing and debugging them. (20 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC-06:00\n",
      "3\n",
      "2019-09-04 03:21:39-06:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from calendar import month_abbr\n",
    "\n",
    "MDT = timezone(timedelta(minutes=-6*60 + 0))\n",
    "print(MDT)\n",
    "def get_datetime(l):\n",
    "    # Extract date, time, and timezone\n",
    "    (date_time, utc_diff) = re.search('(?<= - - \\[).+(?=\\])', l).group().split()\n",
    "    #Break up date_time into date and time\n",
    "    (day_month_year, hour, minute, sec) = re.split(':', date_time) \n",
    "    #Break date up into components\n",
    "    (day, month, year) = re.split('/', day_month_year)\n",
    "    #Convert month to int\n",
    "    month = list(month_abbr).index(month)\n",
    "    #Obtain UTC diff info\n",
    "    utc_diff_hour = int(utc_diff[:3])\n",
    "    utc_diff_mins = int(utc_diff[0] + utc_diff[3:])\n",
    "    #Calculate time in mdt  \n",
    "    time_mdt = datetime(day=int(day), month=month, year=int(year), hour=int(hour), minute=int(minute), \n",
    "                        second=int(sec)) - timedelta(minutes=utc_diff_hour*60 + utc_diff_mins) + timedelta(minutes=-6*60 + 0)\n",
    "                        \n",
    "    time_mdt = time_mdt.replace(tzinfo=MDT)\n",
    "    return time_mdt\n",
    "\n",
    "#Actual\n",
    "print(get_datetime(l).hour)  \n",
    "# Expected: \n",
    "print(datetime(2019, 9, 4, 3, 21, 39, tzinfo=timezone(timedelta(days=-1, seconds=64800))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the log file line by line and use the `get_datetime` and `get_http_info` functions above to calculate the used bandwidth of the server (the sum of all the response sizes) per hour. Use a `dict` or a `defaultdict`. (15 points)\n",
    "\n",
    "For example if there are 4 logs like:\n",
    "\n",
    "    Sep 4 14:20 .... 65bytes\n",
    "    Sep 4 14:35 .... 80bytes\n",
    "    Sep 4 15:01 .... 44bytes\n",
    "    Sep 5 18:20 .... 40bytes\n",
    "\n",
    "The result will be like:\n",
    "\n",
    "    Sep 4 14:00  145\n",
    "    Sep 4 15:00  44\n",
    "    Sep 5 18:00  40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 7:00    49130  bytes\n",
      "2019-07-20 8:00    40469  bytes\n",
      "2019-07-20 9:00    43556  bytes\n",
      "2019-07-20 10:00    82526  bytes\n",
      "2019-07-20 11:00    56328  bytes\n",
      "2019-07-20 12:00    98862  bytes\n",
      "2019-07-20 13:00    119679  bytes\n",
      "2019-07-20 14:00    57126  bytes\n",
      "2019-07-20 15:00    135680  bytes\n",
      "2019-07-20 16:00    48710  bytes\n",
      "2019-07-20 17:00    45631  bytes\n",
      "2019-07-20 18:00    9805  bytes\n",
      "2019-07-20 19:00    3569  bytes\n",
      "2019-07-20 20:00    36087  bytes\n",
      "2019-07-20 21:00    55406  bytes\n",
      "2019-07-20 22:00    68764  bytes\n",
      "2019-07-20 23:00    30101  bytes\n",
      "2019-07-21 0:00    83251  bytes\n",
      "2019-07-21 1:00    77896  bytes\n",
      "2019-07-21 2:00    65166  bytes\n",
      "2019-07-21 3:00    66326  bytes\n",
      "2019-07-21 4:00    86495  bytes\n",
      "2019-07-21 5:00    76888  bytes\n",
      "2019-07-21 6:00    65378  bytes\n",
      "2019-07-21 7:00    116337  bytes\n",
      "2019-07-21 8:00    55348  bytes\n",
      "2019-07-21 9:00    40975  bytes\n",
      "2019-07-21 10:00    51204  bytes\n",
      "2019-07-21 11:00    56527  bytes\n",
      "2019-07-21 12:00    50933  bytes\n",
      "2019-07-21 13:00    29773  bytes\n",
      "2019-07-21 14:00    80279  bytes\n",
      "2019-07-21 15:00    68270  bytes\n",
      "2019-07-21 16:00    12714  bytes\n",
      "2019-07-21 17:00    6414  bytes\n",
      "2019-07-21 18:00    9186  bytes\n",
      "2019-07-21 19:00    4356  bytes\n",
      "2019-07-21 20:00    17380  bytes\n",
      "2019-07-21 21:00    20251  bytes\n",
      "2019-07-21 22:00    25015  bytes\n",
      "2019-07-21 23:00    25326  bytes\n",
      "2019-07-22 0:00    21912  bytes\n",
      "2019-07-22 1:00    27967  bytes\n",
      "2019-07-22 2:00    56146  bytes\n",
      "2019-07-22 3:00    100773  bytes\n",
      "2019-07-22 4:00    87930  bytes\n",
      "2019-07-22 5:00    77608  bytes\n",
      "2019-07-22 6:00    84938  bytes\n",
      "2019-07-22 7:00    114074  bytes\n",
      "2019-07-22 8:00    139403  bytes\n",
      "2019-07-22 9:00    91115  bytes\n",
      "2019-07-22 10:00    85637  bytes\n",
      "2019-07-22 11:00    113985  bytes\n",
      "2019-07-22 12:00    160342  bytes\n",
      "2019-07-22 13:00    147641  bytes\n",
      "2019-07-22 14:00    120636  bytes\n",
      "2019-07-22 15:00    81082  bytes\n",
      "2019-07-22 16:00    66001  bytes\n",
      "2019-07-22 17:00    15379  bytes\n",
      "2019-07-22 18:00    17998  bytes\n",
      "2019-07-22 19:00    10826  bytes\n",
      "2019-07-22 20:00    24312  bytes\n",
      "2019-07-22 21:00    33147  bytes\n",
      "2019-07-22 22:00    24325  bytes\n",
      "2019-07-22 23:00    27712  bytes\n",
      "2019-07-23 0:00    63858  bytes\n",
      "2019-07-23 1:00    44863  bytes\n",
      "2019-07-23 2:00    28687  bytes\n",
      "2019-07-23 3:00    82434  bytes\n",
      "2019-07-23 4:00    91401  bytes\n",
      "2019-07-23 5:00    84796  bytes\n",
      "2019-07-23 6:00    72745  bytes\n",
      "2019-07-23 7:00    35005  bytes\n",
      "2019-07-23 8:00    72550  bytes\n",
      "2019-07-23 9:00    90913  bytes\n",
      "2019-07-23 10:00    86130  bytes\n",
      "2019-07-23 11:00    73650  bytes\n",
      "2019-07-23 12:00    58672  bytes\n",
      "2019-07-23 13:00    95681  bytes\n",
      "2019-07-23 14:00    72453  bytes\n",
      "2019-07-23 15:00    47086  bytes\n",
      "2019-07-23 16:00    7968  bytes\n",
      "2019-07-23 17:00    4677  bytes\n",
      "2019-07-23 18:00    10726  bytes\n",
      "2019-07-23 19:00    9506  bytes\n",
      "2019-07-23 20:00    6394  bytes\n",
      "2019-07-23 21:00    25995  bytes\n",
      "2019-07-23 22:00    64144  bytes\n",
      "2019-07-23 23:00    24542  bytes\n",
      "2019-07-24 0:00    29869  bytes\n",
      "2019-07-24 1:00    39192  bytes\n",
      "2019-07-24 2:00    44674  bytes\n",
      "2019-07-24 3:00    80986  bytes\n",
      "2019-07-24 4:00    50236  bytes\n",
      "2019-07-24 5:00    100834  bytes\n",
      "2019-07-24 6:00    57701  bytes\n",
      "2019-07-24 7:00    58621  bytes\n",
      "2019-07-24 8:00    75741  bytes\n",
      "2019-07-24 9:00    20267  bytes\n",
      "2019-07-24 10:00    48225  bytes\n",
      "2019-07-24 11:00    45402  bytes\n",
      "2019-07-24 12:00    52613  bytes\n",
      "2019-07-24 13:00    113918  bytes\n",
      "2019-07-24 14:00    58613  bytes\n",
      "2019-07-24 15:00    75026  bytes\n",
      "2019-07-24 16:00    107463  bytes\n",
      "2019-07-24 17:00    6480  bytes\n",
      "2019-07-24 18:00    3702  bytes\n",
      "2019-07-24 19:00    9946  bytes\n",
      "2019-07-24 20:00    11825  bytes\n",
      "2019-07-24 21:00    13964  bytes\n",
      "2019-07-24 22:00    15936  bytes\n",
      "2019-07-24 23:00    25976  bytes\n",
      "2019-07-25 0:00    38511  bytes\n",
      "2019-07-25 1:00    44458  bytes\n",
      "2019-07-25 2:00    50009  bytes\n",
      "2019-07-25 3:00    74315  bytes\n",
      "2019-07-25 4:00    48235  bytes\n",
      "2019-07-25 5:00    27275  bytes\n",
      "2019-07-25 6:00    38408  bytes\n",
      "2019-07-25 7:00    35241  bytes\n",
      "2019-07-25 8:00    42373  bytes\n",
      "2019-07-25 9:00    62963  bytes\n",
      "2019-07-25 10:00    28674  bytes\n",
      "2019-07-25 11:00    77992  bytes\n",
      "2019-07-25 12:00    45697  bytes\n",
      "2019-07-25 13:00    48120  bytes\n",
      "2019-07-25 14:00    16894  bytes\n",
      "2019-07-25 15:00    11946  bytes\n",
      "2019-07-25 16:00    9182  bytes\n",
      "2019-07-25 17:00    4174  bytes\n",
      "2019-07-25 18:00    5998  bytes\n",
      "2019-07-25 19:00    22558  bytes\n",
      "2019-07-25 20:00    4467  bytes\n",
      "2019-07-25 21:00    13011  bytes\n",
      "2019-07-25 22:00    15906  bytes\n",
      "2019-07-25 23:00    11325  bytes\n",
      "2019-07-26 0:00    47222  bytes\n",
      "2019-07-26 1:00    47935  bytes\n",
      "2019-07-26 2:00    40544  bytes\n",
      "2019-07-26 3:00    72866  bytes\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "logFileHandle = open('log.txt')\n",
    "\n",
    "totalBytes = 0\n",
    "currHour = -1\n",
    "prevHour = -1\n",
    "currDay = -1\n",
    "prevDay = -1\n",
    "hourlyEntries = dict()\n",
    "\n",
    "for line in logFileHandle:\n",
    "    respSize = get_http_info(line)[3]\n",
    "    dateTimeInfo = get_datetime(line)\n",
    "    currDay = dateTimeInfo.date()\n",
    "    currHour = dateTimeInfo.hour\n",
    "\n",
    "    if prevHour == -1:\n",
    "        prevHour = dateTimeInfo.hour\n",
    "        prevDay = dateTimeInfo.date()\n",
    "        totalBytes = totalBytes + respSize\n",
    "    elif (currDay == prevDay) and (currHour == prevHour):\n",
    "        totalBytes = totalBytes + respSize\n",
    "        prevHour = currHour\n",
    "        prevDay = currDay\n",
    "    else:\n",
    "        hourlyEntries[str(prevDay) + ' ' + str(prevHour) + ':00'] = totalBytes\n",
    "        totalBytes = 0 + respSize;\n",
    "        prevHour = currHour\n",
    "        prevDay = currDay\n",
    "    \n",
    "#Verify\n",
    "for entry in hourlyEntries:\n",
    "    print(entry, '  ', hourlyEntries[entry], ' bytes')   \n",
    "    \n",
    "# No specific format for the output is expected\n",
    "# However the data will be something like:\n",
    "#  2019, 7, 20 07:00    49130 bytes\n",
    "#  2019, 7, 20 08:00    40469 bytes\n",
    "#  2019, 7, 20 09:00    43556 bytes\n",
    "#  2019, 7, 20 10:00    82526 bytes .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-09-04'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Http method\n",
    "    http_method = re.search('(?<=\\] \")\\S+(?= /)', l)\n",
    "    if (http_method == None):\n",
    "        print('Bad format in line: ' + line)\n",
    "        http_method = '';\n",
    "    else:\n",
    "        http_method = http_method.group()\n",
    "    \n",
    "    #Url method\n",
    "    url = re.search('((?<=POST )|(?<=GET ))\\S+(?= HTTP/)', l)\n",
    "    if (url == None):\n",
    "        print('Bad format in line: ' + line)\n",
    "        url = '';\n",
    "    else:\n",
    "        url = url.group()\n",
    "    \n",
    "    #Response method\n",
    "    resp_sz = re.search('(?<=\\d{3} )\\d+(?= \"-\" )', l)\n",
    "    if resp_sz == None:\n",
    "        print('Bad format in line: ' + line)\n",
    "        resp_sz = 0;\n",
    "    else:\n",
    "        resp_sz = int(resp_sz.group())\n",
    "    \n",
    "    #Response code\n",
    "    resp_code = re.search('\\d{3}(?= ' + str(resp_sz) + ' \"-\")', l)\n",
    "    if resp_code == None:\n",
    "        print('Bad format in line: ' + line)\n",
    "        resp_code = -1;\n",
    "    else:\n",
    "        resp_code = int(resp_code.group())\n",
    "    \n",
    "    return (http_method, url, resp_code, resp_sz)\n",
    "\n",
    "print(get_http_info(l))  # Expected: ('POST', '/v1/crash-report/incident/report/', 200, 65)\n",
    "# Please note that the last two numbers are converted to integers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
